{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spark_session import LocalSparkSession\n",
    "from dataset import Dataset\n",
    "from mr_id3 import MapReduceIDR3\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from log import log\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "num_fields = [\n",
    "    'age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss',\n",
    "    'hours_per_week', ]\n",
    "\n",
    "categorical_fields = [\n",
    "    'workclass', 'education',\n",
    "    'marital_status', 'occupation', 'relationship',\n",
    "    'race', 'sex', 'native_country', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "target = 'label'\n",
    "filename = 'dataset/adult.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "clusters = [2, 4, ]  # list(range(1, 2))\n",
    "multiply = [5, ]  # list(range(1, 2))\n",
    "metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/13 10:30:35 WARN Utils: Your hostname, Mac-Pro-de-MARCELO.local resolves to a loopback address: 127.0.0.1; using 192.168.0.62 instead (on interface en2)\n",
      "22/10/13 10:30:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/13 10:30:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/13 10:30:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = LocalSparkSession(2)\n",
    "spark.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 10:30:37,876 [INFO] Dataset : Starting\n",
      "2022-10-13 10:30:37,878 [INFO] Dataset : Loading Dataset /Users/marcelovasconcellos/PycharmProjects/8INF919_Devoir1_Classification-distribuee-par-arbre-de-decision/dataset/adult.data\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(spark.spark, filename, num_fields, categorical_fields, target)\n",
    "dataset.load()\n",
    "dataset.string_indexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\n",
      "\u001B[0;31mZeroDivisionError\u001B[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'age',\n",
       " 'fnlwgt',\n",
       " 'education_num',\n",
       " 'capital_gain',\n",
       " 'capital_loss',\n",
       " 'hours_per_week',\n",
       " 'workclass_idx',\n",
       " 'education_idx',\n",
       " 'marital_status_idx',\n",
       " 'occupation_idx',\n",
       " 'relationship_idx',\n",
       " 'race_idx',\n",
       " 'sex_idx',\n",
       " 'native_country_idx']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+\n",
      "|label|age|fnlwgt|education_num|capital_gain|capital_loss|hours_per_week|workclass_idx|education_idx|marital_status_idx|occupation_idx|relationship_idx|race_idx|sex_idx|native_country_idx|\n",
      "+-----+---+------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+\n",
      "|    0| 39| 77516|           13|        2174|           0|            40|            4|            2|                 1|             3|               1|       0|      0|                 0|\n",
      "|    0| 50| 83311|           13|           0|           0|            13|            1|            2|                 0|             2|               0|       0|      0|                 0|\n",
      "+-----+---+------+-------------+------------+------------+--------------+-------------+-------------+------------------+--------------+----------------+--------+-------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql.types import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[39.0,77516.0,13....|\n",
      "|    0|[50.0,83311.0,13....|\n",
      "|    0|[38.0,215646.0,9....|\n",
      "|    0|[53.0,234721.0,7....|\n",
      "|    0|[28.0,338409.0,13...|\n",
      "|    0|[37.0,284582.0,14...|\n",
      "|    0|[49.0,160187.0,5....|\n",
      "|    1|[52.0,209642.0,9....|\n",
      "|    1|[31.0,45781.0,14....|\n",
      "|    1|[42.0,159449.0,13...|\n",
      "|    1|[37.0,280464.0,10...|\n",
      "|    1|[30.0,141297.0,13...|\n",
      "|    0|[23.0,122272.0,13...|\n",
      "|    0|[32.0,205019.0,12...|\n",
      "|    1|[40.0,121772.0,11...|\n",
      "|    0|[34.0,245487.0,4....|\n",
      "|    0|[25.0,176756.0,9....|\n",
      "|    0|[32.0,186824.0,9....|\n",
      "|    0|[38.0,28887.0,7.0...|\n",
      "|    1|[43.0,292175.0,14...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.rdd.map(lambda r: Row(\n",
    "        label = r.label,\n",
    "#         age = r.age,\n",
    "#         fnlwgt = r.fnlwgt,\n",
    "#         education_num = r.education_num,\n",
    "#         capital_gain = r.capital_gain,\n",
    "#         capital_loss = r.capital_loss,\n",
    "#         hours_per_week = r.hours_per_week,\n",
    "#         workclass_idx = r.workclass_idx,\n",
    "#         education_idx = r.education_idx,\n",
    "#         marital_status_idx = r.marital_status_idx,\n",
    "#         occupation_idx = r.occupation_idx,\n",
    "#         relationship_idx = r.relationship_idx,\n",
    "#         race_idx = r.race_idx,\n",
    "#         sex_idx = r.sex_idx,\n",
    "#         native_country_idx = r.native_country_idx,\n",
    "        features = Vectors.dense(\n",
    "            r.age, \n",
    "            r.fnlwgt, \n",
    "            r.education_num, \n",
    "            r.capital_gain, \n",
    "            r.capital_loss, \n",
    "            r.hours_per_week, \n",
    "            r.workclass_idx, \n",
    "            r.education_idx, \n",
    "            r.marital_status_idx, \n",
    "            r.occupation_idx, \n",
    "            r.relationship_idx, \n",
    "            r.race_idx, \n",
    "            r.sex_idx, \n",
    "            r.native_country_idx\n",
    "        ))\n",
    "        ).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.map(lambda r: Row(\n",
    "                    label = r.label,\n",
    "                    age = r.age,\n",
    "                    features = Vectors.dense(r.label, r.age))\n",
    "                    ).toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"features\", Vectors.dense([df.label, df.age])) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.string_indexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.assemble_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3 = MapReduceIDR3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.set_labeled_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.dataset.df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.training_data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.model = DecisionTree.trainClassifier(\n",
    "            mr_id3.training_data,\n",
    "            numClasses=2,\n",
    "            categoricalFeaturesInfo={},\n",
    "            impurity='entropy',\n",
    "            maxDepth=5,\n",
    "            maxBins=42,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.predictions = mr_id3.model.predict(\n",
    "            mr_id3.test_data.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_and_predictions = mr_id3.test_data.map(\n",
    "            lambda lp: lp.label).zip(mr_id3.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = labels_and_predictions.toDF()\n",
    "dfl.printSchema()\n",
    "dfl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0 #labels_and_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.errors = labels_and_predictions.filter(\n",
    "            lambda lp: lp[0] != lp[1]).count() / float(mr_id3.test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(labels_and_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_id3 = MapReduceIDR3(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "for n_clusters in clusters:\n",
    "    for m_factor in multiply:\n",
    "        spark = LocalSparkSession(n_clusters)\n",
    "        spark.start()\n",
    "\n",
    "        dataset = Dataset(spark.spark, filename, num_fields, categorical_fields, target)\n",
    "        dataset.load()\n",
    "        dataset.string_indexer()\n",
    "        dataset.multiply_dataset(m_factor)\n",
    "\n",
    "        mr_id3 = MapReduceIDR3(dataset)\n",
    "        mr_id3.set_labeled_point()\n",
    "        mr_id3.split()\n",
    "        mr_id3.train()\n",
    "        mr_id3.predict()\n",
    "        \n",
    "        metric = mr_id3.get_metrics()\n",
    "        metric['length_rows'] = dataset.df.count()\n",
    "        metric['m_factor'] = m_factor\n",
    "        metric['n_clusters'] = n_clusters\n",
    "        metrics.append(metric)\n",
    "\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "df = pd.DataFrame.from_dict(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
